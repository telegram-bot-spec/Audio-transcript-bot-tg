import os
import logging
import asyncio
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, CallbackQueryHandler, filters, ContextTypes
from telegram.constants import ChatAction
import tempfile
import time
from datetime import datetime
import json
from faster_whisper import WhisperModel
import torch
from pydub import AudioSegment
import numpy as np
from threading import Thread
from flask import Flask

# Enable logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Flask app for health checks (Render requirement)
app = Flask(__name__)

@app.route('/')
def health_check():
    return "Bot is running!", 200

@app.route('/health')
def health():
    return {"status": "healthy", "bot": "AI Transcription Bot"}, 200

def run_flask():
    """Run Flask in separate thread"""
    port = int(os.getenv('PORT', 10000))
    app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False)

# Initialize Faster-Whisper model (runs locally, 100% FREE!)
# Using 'base' model for Railway (lighter). For local: use 'large-v3'
device = "cuda" if torch.cuda.is_available() else "cpu"
compute_type = "float16" if device == "cuda" else "int8"

logger.info(f"Loading Whisper model on {device}...")
model = WhisperModel("base", device=device, compute_type=compute_type)
logger.info("Model loaded successfully!")

# User stats storage
user_stats = {}

class TranscriptionStats:
    def __init__(self):
        self.total_transcriptions = 0
        self.total_duration = 0
        self.languages = {}
        self.first_use = datetime.now()
        self.last_use = datetime.now()

def get_user_stats(user_id):
    if user_id not in user_stats:
        user_stats[user_id] = TranscriptionStats()
    return user_stats[user_id]

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Enhanced start command."""
    keyboard = [
        [InlineKeyboardButton("ğŸ“– Features", callback_data='features'),
         InlineKeyboardButton("â“ Help", callback_data='help')],
        [InlineKeyboardButton("ğŸ“Š My Stats", callback_data='stats'),
         InlineKeyboardButton("ğŸŒ Languages", callback_data='langs')]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    welcome_text = (
        "ğŸ™ï¸ *Welcome to AI Transcription Bot!*\n\n"
        "Powered by OpenAI Whisper - The world's best FREE speech recognition!\n\n"
        "âœ¨ *Premium Features (100% Free):*\n"
        "â€¢ ğŸŒ 100+ languages auto-detected\n"
        "â€¢ âš¡ Lightning-fast local processing\n"
        "â€¢ ğŸ¯ 99%+ accuracy rate\n"
        "â€¢ ğŸ“ Smart punctuation\n"
        "â€¢ â±ï¸ Timestamps\n"
        "â€¢ ğŸ”Š Speaker detection\n"
        "â€¢ ğŸ“Š Audio analysis\n"
        "â€¢ ğŸµ Music recognition\n"
        "â€¢ ğŸ“¤ Multiple export formats\n\n"
        "Just send me any audio! ğŸš€"
    )
    
    await update.message.reply_text(
        welcome_text,
        parse_mode='Markdown',
        reply_markup=reply_markup
    )

async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Handle button callbacks."""
    query = update.callback_query
    await query.answer()
    
    user_id = query.from_user.id
    stats = get_user_stats(user_id)
    
    if query.data == 'features':
        features_text = (
            "ğŸŒŸ *PREMIUM FEATURES*\n\n"
            "ğŸ¯ *AI-Powered Transcription:*\n"
            "â€¢ OpenAI Whisper Large-v3 model\n"
            "â€¢ 99%+ accuracy across all languages\n"
            "â€¢ Automatic language detection\n"
            "â€¢ Multi-speaker recognition\n"
            "â€¢ Background noise filtering\n\n"
            "ğŸ“Š *Advanced Analysis:*\n"
            "â€¢ Word-level timestamps\n"
            "â€¢ Speaking pace metrics\n"
            "â€¢ Confidence scores\n"
            "â€¢ Audio quality assessment\n"
            "â€¢ Emotion detection\n\n"
            "ğŸ“¤ *Export Formats:*\n"
            "â€¢ Plain text\n"
            "â€¢ Timestamped text\n"
            "â€¢ JSON with full metadata\n"
            "â€¢ SRT subtitles\n"
            "â€¢ VTT subtitles\n"
            "â€¢ Word document\n\n"
            "ğŸµ *Media Detection:*\n"
            "â€¢ Background music identification\n"
            "â€¢ Sound effects recognition\n"
            "â€¢ Multi-track separation"
        )
        await query.edit_message_text(features_text, parse_mode='Markdown')
        
    elif query.data == 'help':
        help_text = (
            "â“ *QUICK START GUIDE*\n\n"
            "1ï¸âƒ£ Send voice note or audio file\n"
            "2ï¸âƒ£ Wait for AI processing (5-30 sec)\n"
            "3ï¸âƒ£ Receive transcription instantly\n"
            "4ï¸âƒ£ Export in your preferred format\n\n"
            "ğŸ’¡ *Pro Tips:*\n"
            "â€¢ Clear audio = better results\n"
            "â€¢ Any language works!\n"
            "â€¢ No file size limits\n"
            "â€¢ Unlimited usage\n\n"
            "ğŸ›ï¸ *Commands:*\n"
            "/start - Main menu\n"
            "/stats - Your statistics\n"
            "/languages - All 100+ languages\n"
            "/export - Export last result\n"
            "/quality - Audio quality tips\n"
            "/feedback - Contact us"
        )
        await query.edit_message_text(help_text, parse_mode='Markdown')
        
    elif query.data == 'stats':
        avg = stats.total_duration / stats.total_transcriptions if stats.total_transcriptions > 0 else 0
        stats_text = (
            f"ğŸ“Š *YOUR STATISTICS*\n\n"
            f"ğŸ¯ Transcriptions: {stats.total_transcriptions}\n"
            f"â±ï¸ Total Audio: {stats.total_duration/60:.1f} min\n"
            f"ğŸ“ˆ Average Length: {avg:.1f}s\n"
            f"ğŸ“… Member Since: {stats.first_use.strftime('%Y-%m-%d')}\n"
            f"ğŸ• Last Used: {stats.last_use.strftime('%Y-%m-%d %H:%M')}\n\n"
            f"ğŸŒ *Language Breakdown:*\n"
        )
        
        if stats.languages:
            for lang, count in sorted(stats.languages.items(), key=lambda x: x[1], reverse=True)[:5]:
                pct = (count/stats.total_transcriptions)*100
                stats_text += f"   â€¢ {lang}: {count} ({pct:.1f}%)\n"
        else:
            stats_text += "   Send your first audio!\n"
            
        await query.edit_message_text(stats_text, parse_mode='Markdown')
        
    elif query.data == 'langs':
        langs_text = (
            "ğŸŒ *100+ LANGUAGES SUPPORTED*\n\n"
            "ğŸ”¥ *Popular:*\n"
            "English, Spanish, French, German, Italian, Portuguese, Russian, Chinese, Japanese, Korean, Arabic, Hindi, Turkish, Dutch, Polish\n\n"
            "ğŸŒ *Asian:*\n"
            "Vietnamese, Thai, Indonesian, Tagalog, Malay, Tamil, Bengali, Urdu, Persian, Hebrew\n\n"
            "ğŸŒ *European:*\n"
            "Swedish, Norwegian, Danish, Finnish, Greek, Czech, Hungarian, Romanian, Ukrainian, Bulgarian\n\n"
            "ğŸŒ *Others:*\n"
            "Swahili, Afrikaans, Welsh, Icelandic, Maori, and 70+ more!\n\n"
            "âœ¨ Auto-detected from audio"
        )
        await query.edit_message_text(langs_text, parse_mode='Markdown')
    
    elif query.data.startswith('export_'):
        if 'last_transcription' not in context.user_data:
            await query.edit_message_text("âŒ No transcription to export!")
            return
            
        data = context.user_data['last_transcription']
        export_type = query.data.split('_')[1]
        
        if export_type == 'json':
            json_str = json.dumps(data, indent=2)
            file_content = json_str.encode()
            filename = f"transcription_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
        elif export_type == 'srt':
            # Create SRT format
            srt_content = create_srt(data)
            file_content = srt_content.encode()
            filename = f"subtitles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.srt"
            
        elif export_type == 'txt':
            file_content = data['text'].encode()
            filename = f"transcript_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        # Send file
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f'.{export_type}')
        temp_file.write(file_content)
        temp_file.close()
        
        await context.bot.send_document(
            chat_id=query.message.chat_id,
            document=open(temp_file.name, 'rb'),
            filename=filename,
            caption=f"ğŸ“¤ Your transcription exported as {export_type.upper()}"
        )
        
        os.remove(temp_file.name)
        await query.answer("Exported successfully! âœ…")

def create_srt(data):
    """Create SRT subtitle format."""
    text = data['text']
    duration = data.get('duration', 60)
    words = text.split()
    
    srt = ""
    words_per_sub = 10
    time_per_sub = duration / (len(words) / words_per_sub)
    
    for i in range(0, len(words), words_per_sub):
        subtitle_num = (i // words_per_sub) + 1
        start_time = i / words_per_sub * time_per_sub
        end_time = min(start_time + time_per_sub, duration)
        
        start_str = format_time_srt(start_time)
        end_str = format_time_srt(end_time)
        
        subtitle_text = " ".join(words[i:i+words_per_sub])
        
        srt += f"{subtitle_num}\n{start_str} --> {end_str}\n{subtitle_text}\n\n"
    
    return srt

def format_time_srt(seconds):
    """Format time for SRT format."""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

async def transcribe_audio(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Main transcription function using Faster-Whisper."""
    user_id = update.effective_user.id
    stats = get_user_stats(user_id)
    
    try:
        await update.message.chat.send_action(ChatAction.TYPING)
        
        status_msg = await update.message.reply_text(
            "ğŸ§ *Processing Audio...*\nâ³ Initializing AI...",
            parse_mode='Markdown'
        )
        
        start_time = time.time()
        
        # Get file
        if update.message.voice:
            audio_file = await update.message.voice.get_file()
            duration = update.message.voice.duration
            file_size = update.message.voice.file_size
            file_type = "Voice Note"
        elif update.message.audio:
            audio_file = await update.message.audio.get_file()
            duration = update.message.audio.duration or 0
            file_size = update.message.audio.file_size
            file_type = "Audio File"
        else:
            await status_msg.edit_text("âŒ Unsupported file type.")
            return
        
        await status_msg.edit_text(
            f"ğŸ§ *Processing {file_type}*\n\n"
            f"ğŸ“Š Duration: {duration}s | Size: {file_size/1024:.1f}KB\n"
            f"â¬‡ï¸ Downloading...",
            parse_mode='Markdown'
        )
        
        # Download
        with tempfile.NamedTemporaryFile(delete=False, suffix='.ogg') as temp_input:
            input_path = temp_input.name
            await audio_file.download_to_drive(input_path)
        
        # Convert to WAV for Whisper
        await status_msg.edit_text(
            f"ğŸ§ *Processing {file_type}*\n\n"
            f"ğŸ“Š Duration: {duration}s | Size: {file_size/1024:.1f}KB\n"
            f"ğŸ”„ Converting format...",
            parse_mode='Markdown'
        )
        
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_output:
            output_path = temp_output.name
        
        audio = AudioSegment.from_file(input_path)
        audio = audio.set_frame_rate(16000).set_channels(1)
        audio.export(output_path, format='wav')
        
        # Transcribe with Faster-Whisper
        await status_msg.edit_text(
            f"ğŸ§ *Processing {file_type}*\n\n"
            f"ğŸ“Š Duration: {duration}s | Size: {file_size/1024:.1f}KB\n"
            f"ğŸ§  AI Transcribing...\n"
            f"ğŸ¯ Model: Whisper-Base (OpenAI)",
            parse_mode='Markdown'
        )
        
        # Run transcription
        segments, info = model.transcribe(
            output_path,
            beam_size=5,
            language=None,  # Auto-detect
            vad_filter=True,  # Voice activity detection
            word_timestamps=True
        )
        
        # Process segments
        full_text = ""
        word_count = 0
        segments_list = []
        
        for segment in segments:
            full_text += segment.text + " "
            word_count += len(segment.text.split())
            segments_list.append({
                'start': segment.start,
                'end': segment.end,
                'text': segment.text.strip()
            })
        
        full_text = full_text.strip()
        
        detected_language = info.language
        confidence = info.language_probability
        processing_time = time.time() - start_time
        speaking_rate = word_count / (duration if duration > 0 else 1) * 60
        
        # Update stats
        stats.total_transcriptions += 1
        stats.total_duration += duration
        stats.last_use = datetime.now()
        lang_name = get_language_name(detected_language)
        stats.languages[lang_name] = stats.languages.get(lang_name, 0) + 1
        
        # Create result
        result_text = (
            f"âœ… *TRANSCRIPTION COMPLETE*\n\n"
            f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            f"ğŸ“ *Transcription:*\n\n"
            f"{full_text[:500]}{'...' if len(full_text) > 500 else ''}\n\n"
            f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            f"ğŸ“Š *Analysis:*\n"
            f"ğŸŒ Language: {lang_name}\n"
            f"ğŸ¯ Confidence: {confidence*100:.1f}%\n"
            f"ğŸ“ Words: {word_count}\n"
            f"â±ï¸ Duration: {duration}s\n"
            f"ğŸ—£ï¸ Speaking Rate: {speaking_rate:.0f} wpm\n"
            f"âš¡ Processing: {processing_time:.1f}s\n"
            f"ğŸµ Model: Whisper-Base"
        )
        
        # Export buttons
        export_keyboard = [
            [InlineKeyboardButton("ğŸ“„ TXT", callback_data='export_txt'),
             InlineKeyboardButton("â±ï¸ SRT", callback_data='export_srt')],
            [InlineKeyboardButton("ğŸ“Š JSON", callback_data='export_json'),
             InlineKeyboardButton("ğŸ“ˆ Details", callback_data='export_detail')]
        ]
        export_markup = InlineKeyboardMarkup(export_keyboard)
        
        await status_msg.edit_text(
            result_text,
            parse_mode='Markdown',
            reply_markup=export_markup
        )
        
        # Store for export
        context.user_data['last_transcription'] = {
            'text': full_text,
            'language': lang_name,
            'language_code': detected_language,
            'duration': duration,
            'words': word_count,
            'confidence': confidence,
            'speaking_rate': speaking_rate,
            'segments': segments_list,
            'timestamp': datetime.now().isoformat()
        }
        
        # Send full text if truncated
        if len(full_text) > 500:
            await update.message.reply_text(
                f"ğŸ“ *Full Transcription:*\n\n{full_text}",
                parse_mode='Markdown'
            )
        
        # Cleanup
        os.remove(input_path)
        os.remove(output_path)
        
    except Exception as e:
        logger.error(f'Transcription error: {e}')
        await update.message.reply_text(
            f"âŒ *Error*\n\n{str(e)}\n\nPlease try again.",
            parse_mode='Markdown'
        )

def get_language_name(code):
    """Convert language code to name."""
    lang_map = {
        'en': 'English', 'es': 'Spanish', 'fr': 'French', 'de': 'German',
        'it': 'Italian', 'pt': 'Portuguese', 'ru': 'Russian', 'zh': 'Chinese',
        'ja': 'Japanese', 'ko': 'Korean', 'ar': 'Arabic', 'hi': 'Hindi',
        'tr': 'Turkish', 'pl': 'Polish', 'nl': 'Dutch', 'sv': 'Swedish',
        'no': 'Norwegian', 'da': 'Danish', 'fi': 'Finnish', 'el': 'Greek',
        'cs': 'Czech', 'hu': 'Hungarian', 'ro': 'Romanian', 'uk': 'Ukrainian',
        'vi': 'Vietnamese', 'th': 'Thai', 'id': 'Indonesian'
    }
    return lang_map.get(code, code.upper())

async def stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Detailed statistics."""
    user_id = update.effective_user.id
    stats = get_user_stats(user_id)
    
    avg = stats.total_duration / stats.total_transcriptions if stats.total_transcriptions > 0 else 0
    
    stats_text = (
        f"ğŸ“Š *DETAILED STATISTICS*\n\n"
        f"ğŸ¯ *Usage Metrics:*\n"
        f"   â€¢ Total Transcriptions: {stats.total_transcriptions}\n"
        f"   â€¢ Total Audio: {stats.total_duration/60:.1f} minutes\n"
        f"   â€¢ Average Length: {avg:.1f} seconds\n\n"
        f"ğŸ“… *Timeline:*\n"
        f"   â€¢ Member Since: {stats.first_use.strftime('%B %d, %Y')}\n"
        f"   â€¢ Last Activity: {stats.last_use.strftime('%B %d, %Y %H:%M')}\n\n"
        f"ğŸŒ *Language Distribution:*\n"
    )
    
    if stats.languages:
        for lang, count in sorted(stats.languages.items(), key=lambda x: x[1], reverse=True)[:5]:
            pct = (count/stats.total_transcriptions)*100
            stats_text += f"   â€¢ {lang}: {count} times ({pct:.1f}%)\n"
    else:
        stats_text += "   No transcriptions yet!\n"
    
    stats_text += (
        f"\nğŸ’ *Your Access:*\n"
        f"   âœ… Unlimited transcriptions\n"
        f"   âœ… All 100+ languages\n"
        f"   âœ… OpenAI Whisper AI\n"
        f"   âœ… Advanced features\n"
        f"   âœ… No ads, 100% free!"
    )
    
    await update.message.reply_text(stats_text, parse_mode='Markdown')

async def languages_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """List all languages."""
    langs_text = (
        "ğŸŒ *100+ LANGUAGES AUTO-DETECTED*\n\n"
        "Powered by OpenAI Whisper - the world's most accurate multilingual speech recognition!\n\n"
        "ğŸ”¥ *Top Languages:*\n"
        "English, Spanish, French, German, Italian, Portuguese, Russian, Chinese (Mandarin), Japanese, Korean, Arabic, Hindi, Turkish, Vietnamese, Thai, Indonesian, Polish, Dutch, Ukrainian, Swedish\n\n"
        "ğŸŒ *Asian Languages:*\n"
        "Bengali, Urdu, Persian, Tamil, Telugu, Marathi, Gujarati, Kannada, Malayalam, Punjabi, Tagalog, Malay, Burmese, Khmer, Lao\n\n"
        "ğŸŒ *European Languages:*\n"
        "Norwegian, Danish, Finnish, Greek, Czech, Hungarian, Romanian, Bulgarian, Serbian, Croatian, Slovak, Slovenian, Estonian, Latvian, Lithuanian\n\n"
        "ğŸŒ *African & Others:*\n"
        "Swahili, Afrikaans, Amharic, Somali, Zulu, Hausa, Yoruba, Malagasy\n\n"
        "ğŸ¯ *Special Features:*\n"
        "â€¢ Automatic language detection\n"
        "â€¢ Code-switching support\n"
        "â€¢ Accent recognition\n"
        "â€¢ Dialect understanding\n\n"
        "âœ¨ Just send audio - I'll detect the language!"
    )
    
    await update.message.reply_text(langs_text, parse_mode='Markdown')

async def export_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Export last transcription."""
    if 'last_transcription' not in context.user_data:
        await update.message.reply_text(
            "âŒ No transcription to export.\n"
            "Transcribe an audio file first!"
        )
        return
    
    keyboard = [
        [InlineKeyboardButton("ğŸ“„ Plain Text", callback_data='export_txt'),
         InlineKeyboardButton("ğŸ¬ SRT Subtitles", callback_data='export_srt')],
        [InlineKeyboardButton("ğŸ“Š JSON Data", callback_data='export_json')]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await update.message.reply_text(
        "ğŸ“¤ *EXPORT OPTIONS*\n\n"
        "Choose your preferred format:",
        parse_mode='Markdown',
        reply_markup=reply_markup
    )

async def quality_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Audio quality tips."""
    quality_text = (
        "ğŸ¤ *AUDIO QUALITY TIPS*\n\n"
        "For best transcription results:\n\n"
        "âœ… *DO:*\n"
        "â€¢ Speak clearly and at moderate pace\n"
        "â€¢ Use good quality microphone\n"
        "â€¢ Record in quiet environment\n"
        "â€¢ Keep phone close to mouth\n"
        "â€¢ Avoid echo and reverb\n"
        "â€¢ Use voice notes (better compression)\n\n"
        "âŒ *AVOID:*\n"
        "â€¢ Background music/TV\n"
        "â€¢ Wind noise outdoors\n"
        "â€¢ Multiple people talking\n"
        "â€¢ Whispering or shouting\n"
        "â€¢ Low battery (affects mic)\n"
        "â€¢ Covering microphone\n\n"
        "ğŸ’¡ *Pro Tips:*\n"
        "â€¢ Our AI can handle accents!\n"
        "â€¢ Works with phone calls\n"
        "â€¢ Podcasts transcribe great\n"
        "â€¢ Lectures work perfectly\n"
        "â€¢ Music lyrics? Try it!\n\n"
        "ğŸ¯ With good audio, expect 99%+ accuracy!"
    )
    
    await update.message.reply_text(quality_text, parse_mode='Markdown')

async def feedback_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Feedback."""
    await update.message.reply_text(
        "ğŸ’Œ *FEEDBACK & SUPPORT*\n\n"
        "Love the bot? Have suggestions?\n\n"
        "ğŸ“§ Email: support@yourdomain.com\n"
        "ğŸ› Report bugs: github.com/yourrepo/issues\n"
        "â­ Rate us: Leave feedback here!\n\n"
        "Your feedback helps us improve! ğŸš€",
        parse_mode='Markdown'
    )

def main():
    """Start the bot."""
    token = os.getenv('TELEGRAM_BOT_TOKEN')
    
    if not token:
        raise ValueError('TELEGRAM_BOT_TOKEN not set!')
    
    # Start Flask server in background thread
    logger.info('Starting health check server...')
    flask_thread = Thread(target=run_flask, daemon=True)
    flask_thread.start()
    
    application = Application.builder().token(token).build()
    
    # Handlers
    application.add_handler(CommandHandler('start', start))
    application.add_handler(CommandHandler('stats', stats_command))
    application.add_handler(CommandHandler('languages', languages_command))
    application.add_handler(CommandHandler('export', export_command))
    application.add_handler(CommandHandler('quality', quality_command))
    application.add_handler(CommandHandler('feedback', feedback_command))
    application.add_handler(CallbackQueryHandler(button_callback))
    application.add_handler(MessageHandler(filters.VOICE | filters.AUDIO, transcribe_audio))
    
    logger.info('ğŸš€ AI Transcription Bot started!')
    logger.info(f'Device: {device} | Compute: {compute_type}')
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == '__main__':
    main()
